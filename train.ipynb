{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1965598",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export KMP_DUPLICATE_LIB_OK=TRUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b79be36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from txtai.embeddings import Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456a7c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set_version=1.2\n",
    "data_set_file_name = f\"dataset_v{data_set_version}.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc97a111",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172704b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def process_json_dataset(filepath):\n",
    "    \"\"\"\n",
    "    Reads a JSON file, extracts data, and transforms it into a list of dictionaries.\n",
    "\n",
    "    Args:\n",
    "        filepath (str): The path to the JSON file.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries, where each dictionary has \"output\" and \"text_input\" keys.\n",
    "    \"\"\"\n",
    "    try:\n",
    "      with open(filepath, 'r') as f:\n",
    "          data = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {filepath}\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        return []\n",
    "\n",
    "    index_data = []\n",
    "    pair_counter = 0\n",
    "    all_ids = set()\n",
    "    for category, chunks in data.items():\n",
    "        print(f\"Processing category: {category}\")\n",
    "        for item in chunks:\n",
    "            chunk_id_base = item['id']\n",
    "            answer_chunk = item['text']\n",
    "            chunk_metadata = item['metadata']\n",
    "\n",
    "            for i, query in enumerate(item['questions']):\n",
    "                pair_id = f\"{chunk_id_base}_q{i}\" # Generate a unique ID for the pair\n",
    "                pair_counter += 1\n",
    "\n",
    "                # Data object stores the question, the answer, and metadata\n",
    "                data_object = {\n",
    "                    \"text\": query, # This is what txtai will embed by default if 'text' key exists\n",
    "                    \"answer\": answer_chunk,\n",
    "                    \"category\": category,\n",
    "                    \"metadata\": chunk_metadata\n",
    "                }\n",
    "\n",
    "                # Append tuple: (unique_pair_id, data_object_to_index, optional_tags)\n",
    "                # We use the 'question' field for embedding similarity.\n",
    "                index_data.append((pair_id, data_object, None)) # Pass metadata as tags\n",
    "\n",
    "    print(f\"Prepared {len(index_data)} items for indexing.\")\n",
    "    print(index_data)\n",
    "    return index_data\n",
    "\n",
    "\n",
    "\n",
    "dataset_filepath = f\"./data/{data_set_file_name}\"\n",
    "\n",
    "index_data = process_json_dataset(dataset_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c213b2ec",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\n\u001b[32m     34\u001b[39m index_path_qa = \u001b[33m\"\u001b[39m\u001b[33m./mounts/index\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m embeddings = \u001b[43mindexing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex_path_qa\u001b[49m\u001b[43m,\u001b[49m\u001b[43msave\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mindexing\u001b[39m\u001b[34m(filePath, save)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mindexing\u001b[39m(filePath, save=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m      4\u001b[39m \n\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m# Create embeddings in dex with content enabled. The default behavior is to only store indexed vectors.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     embeddings = \u001b[43mEmbeddings\u001b[49m({\u001b[33m\"\u001b[39m\u001b[33mpath\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33msentence-transformers/nli-mpnet-base-v2\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m})\n\u001b[32m      8\u001b[39m     \u001b[38;5;66;03m# Map question to text and store content\u001b[39;00m\n\u001b[32m      9\u001b[39m     embeddings.index(index_data)\n",
      "\u001b[31mNameError\u001b[39m: name 'Embeddings' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def indexing(filePath, save=False):\n",
    "\n",
    "    # Create embeddings in dex with content enabled. The default behavior is to only store indexed vectors.\n",
    "    embeddings = Embeddings({\"path\": \"sentence-transformers/nli-mpnet-base-v2\", \"content\": True})\n",
    "\n",
    "    # Map question to text and store content\n",
    "    embeddings.index(index_data)\n",
    "\n",
    "    if(save):\n",
    "        if not os.path.exists(filePath):\n",
    "            os.makedirs(filePath)\n",
    "        print(f\"Saving QA index to {filePath}...\")\n",
    "        embeddings.save(filePath)\n",
    "        print(\"QA Index saved.\")\n",
    "\n",
    "    try:\n",
    "        # --- !!! ADD THIS CHECK !!! ---\n",
    "        index_count_after = embeddings.count()\n",
    "        print(f\"[indexing function] Count immediately after .index() call: {index_count_after}\")\n",
    "        if index_count_after == 0 and len(index_data) > 0:\n",
    "            print(\"[indexing function] CRITICAL: Index count is 0 but data was provided!\")\n",
    "        # --- END CHECK ---\n",
    "\n",
    "        print(\"Indexing completed successfully in memory (according to try block).\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during embeddings.index(): {e}\")\n",
    "        return None # Return None if indexing failed\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "index_path_qa = \"./index_files\"\n",
    "embeddings = indexing(index_path_qa,save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae684581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'tell me about xcuisite website', 'answer': \"The XCuisite website is an e-commerce platform designed for selling doughnuts. A live demo is available at www.xcuisite.store, or you can click the link in the 'Projects' section.\", 'score': 0.9506585597991943}\n",
      "----------\n",
      "{'text': 'tell me more about xcuisite website', 'answer': \"The XCuisite Website project's technical goal was to deepen understanding of a full-stack website's flow, covering frontend/backend communication, error handling, authN/authZ, database queries, payment processing, scalability, and containerized cloud deployment with SSL and a custom domain.\", 'score': 0.9222586750984192}\n",
      "----------\n",
      "{'text': 'can you show me xcuisite website?', 'answer': \"The XCuisite website is an e-commerce platform designed for selling doughnuts. A live demo is available at www.xcuisite.store, or you can click the link in the 'Projects' section.\", 'score': 0.8420232534408569}\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def find_answer(user_query):\n",
    "    embeddings = Embeddings({\"path\": \"sentence-transformers/nli-mpnet-base-v2\", \"content\": True})\n",
    "    index_path_qa = \"./mounts/index\"\n",
    "    embeddings.load(index_path_qa) # Load the index from the specified path\n",
    "    query_sql = f\"SELECT text, answer, score FROM txtai WHERE similar('{user_query}') LIMIT 3\"\n",
    "    results = embeddings.search(query_sql)\n",
    "    return results\n",
    "\n",
    "def print_ans(search_results):\n",
    "    if search_results:\n",
    "        for result in search_results:\n",
    "            print(result)\n",
    "            print(\"-\" * 10)\n",
    "    else:\n",
    "        print(\"  No similar questions found in the index.\")\n",
    "# Example Usage:\n",
    "user_question = \"Tell me about your xcuisite website\"\n",
    "search_results = find_answer(user_question)\n",
    "print_ans(search_results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c5bb19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
